{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysGenRD All-in-One Notebook\n",
    "필수 기능만 남기고 정리한 실행형 워크플로입니다.\n",
    "- 설정/목표 곡선 구성\n",
    "- SDF + FNO 서러게이트 + Latent Diffusion\n",
    "- Forward 예측 / Reverse 설계(단발+루프)\n",
    "- 평가/요약 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사진 요소 반영 안내\n",
    "요청하신 **사진의 구성 요소**를 노트북에 추가하려면 해당 이미지(또는 요소 목록)를 제공해 주세요.\n",
    "이미지 전달 시 동일 섹션에 UI/도식 요소를 그대로 반영하겠습니다.\n",
    "\n",
    "추가로, 요청에 따라 `Grain (1).ipynb`, `physgenrd.py`, `forward.py` 파일은 삭제했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"3D-PhysGenRD: 3D physics-guided generative reverse design utilities.\n\n이 모듈은 3D SDF 기반 형상 표현, 조건부 latent diffusion,\n물리 surrogate(에이코날/연소면적)와 역설계 최적화를 포함합니다.\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional\n\nimport math\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as nnF\nimport torch.optim as optim\n\ntry:\n    from tqdm.auto import tqdm\nexcept Exception:\n    def tqdm(iterable, **kwargs):\n        return iterable\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept Exception:\n    SummaryWriter = None\n\n\n@dataclass\nclass GrainConfig:\n    seed: int = 1234\n    gamma: float = 1.22\n    gas_constant: float = 355.0\n    chamber_temp: float = 3200.0\n    rho_p: float = 1700.0\n    burn_a: float = 5.0e-5\n    burn_n: float = 0.35\n    pa: float = 101325.0\n    throat_area: float = 3.0e-4\n    length: float = 0.20\n    case_radius: float = 0.10\n    grid_size: int = 48\n    dt: float = 0.001\n    t_end: float = 3.5\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n@dataclass\nclass DiffusionConfig:\n    latent_dim: int = 512\n    time_steps: int = 1000\n    beta_start: float = 1e-4\n    beta_end: float = 2e-2\n    guidance_scale: float = 3.5\n\n\n@dataclass\nclass TrainingConfig:\n    num_iters: int = 1200\n    lr: float = 2e-4\n    clip_grad: float = 0.5\n    target_loading: float = 0.70\n    min_loading: float = 0.35\n    max_loading: float = 1.00\n    loop_steps: int = 3\n    inner_iters: int = 400\n    performance_tol: float = 0.05\n    guidance_scale: float = 4.0\n    pinn_weight: float = 2.5\n    loading_weight: float = 8.0\n    smooth_weight: float = 0.25\n    latent_weight: float = 0.05\n    cond_dropout: float = 0.1\n    log_dir: str = \"out_physgenrd/tb\"\n    log_every: int = 10\n    early_stop_patience: int = 200\n    early_stop_min_delta: float = 1e-4\n    use_tqdm: bool = True\n\n\n\n\nclass EarlyStopping:\n    def __init__(self, patience: int, min_delta: float) -> None:\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best: Optional[float] = None\n        self.counter = 0\n\n    def step(self, value: float) -> bool:\n        if self.best is None or value < self.best - self.min_delta:\n            self.best = value\n            self.counter = 0\n            return False\n        self.counter += 1\n        return self.counter >= self.patience\n\n\ndef init_summary_writer(train_cfg: TrainingConfig, run_name: str) -> Optional[SummaryWriter]:\n    if SummaryWriter is None:\n        return None\n    log_root = Path(train_cfg.log_dir)\n    log_root.mkdir(parents=True, exist_ok=True)\n    run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    return SummaryWriter(log_dir=str(log_root / f\"{run_name}_{run_stamp}\"))\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef make_grid(cfg: GrainConfig) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    x = torch.linspace(-cfg.case_radius, cfg.case_radius, cfg.grid_size, device=cfg.device)\n    y = torch.linspace(-cfg.case_radius, cfg.case_radius, cfg.grid_size, device=cfg.device)\n    z = torch.linspace(-cfg.length / 2, cfg.length / 2, cfg.grid_size, device=cfg.device)\n    zz, yy, xx = torch.meshgrid(z, y, x, indexing=\"ij\")\n    coords = torch.stack([xx, yy, zz], dim=-1)\n    coords_flat = coords.reshape(-1, 3)\n    return xx, yy, zz, coords_flat\n\n\nclass GaussianFourierFeatures(nn.Module):\n    def __init__(self, in_dim: int, out_dim: int, scale: float = 10.0) -> None:\n        super().__init__()\n        self.register_buffer(\"weight\", torch.randn(in_dim, out_dim) * scale)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        proj = x @ self.weight\n        return torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, in_dim: int = 3, num_freqs: int = 6, use_gaussian: bool = True) -> None:\n        super().__init__()\n        self.freq_bands = 2.0 ** torch.linspace(0.0, num_freqs - 1, num_freqs)\n        self.use_gaussian = use_gaussian\n        if use_gaussian:\n            self.gaussian = GaussianFourierFeatures(in_dim, num_freqs)\n        else:\n            self.gaussian = None\n\n    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n        enc = [coords]\n        for d in range(coords.shape[-1]):\n            c = coords[:, d:d + 1]\n            for f in self.freq_bands.to(coords.device):\n                enc.append(torch.sin(f * c))\n                enc.append(torch.cos(f * c))\n        if self.gaussian is not None:\n            enc.append(self.gaussian(coords))\n        return torch.cat(enc, dim=-1)\n\n\nclass SinusoidalTimeEmbedding(nn.Module):\n    def __init__(self, dim: int = 128) -> None:\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        half = self.dim // 2\n        device = t.device\n        freqs = torch.exp(\n            -math.log(10000.0) * torch.arange(0, half, device=device).float() / (half - 1)\n        )\n        args = t * freqs\n        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n        return emb\n\n\nclass MLPBlock(nn.Module):\n    def __init__(self, in_dim: int, out_dim: int, residual: bool = False) -> None:\n        super().__init__()\n        self.residual = residual and in_dim == out_dim\n        self.fc1 = nn.Linear(in_dim, out_dim)\n        self.fc2 = nn.Linear(out_dim, out_dim)\n        self.act = nn.SiLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = self.act(self.fc1(x))\n        out = self.fc2(out)\n        if self.residual:\n            out = out + x\n        return self.act(out)\n\n\nclass CurveEncoder(nn.Module):\n    def __init__(self, in_channels: int = 1, hidden: int = 128, out_dim: int = 256) -> None:\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(in_channels, hidden, kernel_size=5, padding=2),\n            nn.GELU(),\n            nn.Conv1d(hidden, hidden, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(hidden, out_dim),\n            nn.GELU(),\n        )\n\n    def forward(self, curves: torch.Tensor) -> torch.Tensor:\n        return self.net(curves)\n\n\nclass NeuralSDFField(nn.Module):\n    def __init__(\n        self,\n        enc_dim: int,\n        latent_dim: int,\n        cond_dim: int,\n        hidden: int = 512,\n        layers: int = 9,\n    ) -> None:\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.cond_dim = cond_dim\n        self.in_dim = enc_dim + latent_dim + cond_dim\n        self.layers = nn.ModuleList()\n        for i in range(layers):\n            in_features = self.in_dim if i == 0 else hidden\n            self.layers.append(MLPBlock(in_features, hidden, residual=i > 0))\n        self.skip_index = layers // 2\n        self.out = nn.Linear(hidden + self.in_dim, 1)\n\n    def forward(self, enc: torch.Tensor, z: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n        z_expanded = z.unsqueeze(0).expand(enc.shape[0], -1)\n        cond_expanded = cond.unsqueeze(0).expand(enc.shape[0], -1)\n        x = torch.cat([enc, z_expanded, cond_expanded], dim=-1)\n        h = x\n        for idx, layer in enumerate(self.layers):\n            h = layer(h)\n            if idx == self.skip_index:\n                h = torch.cat([h, x], dim=-1)\n        sdf = self.out(h).squeeze(-1)\n        return torch.clamp(sdf, -1.0, 1.0)\n\n\nclass LatentDenoiser(nn.Module):\n    def __init__(self, latent_dim: int, cond_dim: int) -> None:\n        super().__init__()\n        self.time_embed = SinusoidalTimeEmbedding(128)\n        self.time_mlp = nn.Sequential(\n            nn.Linear(128, 256),\n            nn.SiLU(),\n            nn.Linear(256, 256),\n        )\n        self.cond_proj = nn.Linear(cond_dim, 256)\n        self.blocks = nn.Sequential(\n            MLPBlock(latent_dim + 256 + 256, 1024),\n            MLPBlock(1024, 1024, residual=True),\n            MLPBlock(1024, 1024, residual=True),\n        )\n        self.out = nn.Linear(1024, latent_dim)\n\n    def forward(self, z: torch.Tensor, t: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n        if z.dim() == 1:\n            z = z.unsqueeze(0)\n        if cond.dim() == 1:\n            cond = cond.unsqueeze(0)\n        if t.dim() > 1 and t.shape[-1] == 1:\n            t = t.squeeze(-1)\n        if t.dim() == 0:\n            t = t.unsqueeze(0)\n        t_embed = self.time_mlp(self.time_embed(t))\n        cond_embed = self.cond_proj(cond)\n        x = torch.cat([z, cond_embed, t_embed], dim=-1)\n        return self.out(self.blocks(x))\n\n\nclass LatentDiffusion(nn.Module):\n    def __init__(self, cfg: DiffusionConfig, cond_dim: int) -> None:\n        super().__init__()\n        self.cfg = cfg\n        betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.time_steps)\n        alphas = 1.0 - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        self.register_buffer(\"betas\", betas)\n        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1 - alphas_cumprod))\n        self.denoiser = LatentDenoiser(cfg.latent_dim, cond_dim)\n\n    def q_sample(self, z0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n        sqrt_alpha = self.sqrt_alphas_cumprod[t].unsqueeze(-1)\n        sqrt_one_minus = self.sqrt_one_minus_alphas_cumprod[t].unsqueeze(-1)\n        return sqrt_alpha * z0 + sqrt_one_minus * noise\n\n    def predict_eps(self, zt: torch.Tensor, t: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n        time = t.float() / float(self.cfg.time_steps)\n        return self.denoiser(zt, time, cond)\n\n    def p_sample(self, zt: torch.Tensor, t: int, cond: torch.Tensor) -> torch.Tensor:\n        t_tensor = torch.tensor([t], device=zt.device, dtype=torch.long)\n        eps = self.predict_eps(zt, t_tensor, cond)\n        alpha = self.alphas_cumprod[t]\n        alpha_prev = self.alphas_cumprod[t - 1] if t > 0 else torch.tensor(1.0, device=zt.device)\n        pred_x0 = (zt - torch.sqrt(1 - alpha) * eps) / torch.sqrt(alpha)\n        noise = torch.randn_like(zt) if t > 0 else torch.zeros_like(zt)\n        sigma = torch.sqrt((1 - alpha_prev) / (1 - alpha))\n        return torch.sqrt(alpha_prev) * pred_x0 + sigma * noise\n\n    def sample(self, cond: torch.Tensor, guidance_scale: float | None = None) -> torch.Tensor:\n        guidance = guidance_scale if guidance_scale is not None else self.cfg.guidance_scale\n        z = torch.randn(1, self.cfg.latent_dim, device=cond.device)\n        for t in reversed(range(self.cfg.time_steps)):\n            cond_drop = torch.zeros_like(cond)\n            eps_cond = self.predict_eps(z, torch.tensor([t], device=cond.device), cond)\n            eps_uncond = self.predict_eps(z, torch.tensor([t], device=cond.device), cond_drop)\n            eps = eps_uncond + guidance * (eps_cond - eps_uncond)\n            alpha = self.alphas_cumprod[t]\n            alpha_prev = self.alphas_cumprod[t - 1] if t > 0 else torch.tensor(1.0, device=cond.device)\n            pred_x0 = (z - torch.sqrt(1 - alpha) * eps) / torch.sqrt(alpha)\n            noise = torch.randn_like(z) if t > 0 else torch.zeros_like(z)\n            sigma = torch.sqrt((1 - alpha_prev) / (1 - alpha))\n            z = torch.sqrt(alpha_prev) * pred_x0 + sigma * noise\n        return z.squeeze(0)\n\n\nclass PhysicsSurrogate(nn.Module):\n    def __init__(self, grid_size: int) -> None:\n        super().__init__()\n        self.net = FNO3D(in_channels=1, hidden=32, modes=8, layers=3)\n        self.grid_size = grid_size\n\n    def forward(self, sdf_grid: torch.Tensor) -> torch.Tensor:\n        return self.net(sdf_grid)\n\n    def eikonal_residual(self, w: torch.Tensor, spacing: float) -> torch.Tensor:\n        grad = torch.gradient(w, spacing=(spacing, spacing, spacing))\n        grad_norm = torch.sqrt(sum(g**2 for g in grad) + 1e-8)\n        return (grad_norm - 1.0).pow(2).mean()\n\n\nclass SpectralConv3d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, modes: int) -> None:\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes = modes\n        scale = 1 / (in_channels * out_channels)\n        self.weight = nn.Parameter(\n            scale * torch.randn(in_channels, out_channels, modes, modes, modes, dtype=torch.cfloat)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch, channels, nx, ny, nz = x.shape\n        x_ft = torch.fft.rfftn(x, dim=(-3, -2, -1))\n        out_ft = torch.zeros(\n            batch,\n            self.out_channels,\n            nx,\n            ny,\n            nz // 2 + 1,\n            device=x.device,\n            dtype=torch.cfloat,\n        )\n        mx = min(self.modes, nx)\n        my = min(self.modes, ny)\n        mz = min(self.modes, nz // 2 + 1)\n        out_ft[:, :, :mx, :my, :mz] = torch.einsum(\n            \"bixyz,ioxyz->boxyz\", x_ft[:, :, :mx, :my, :mz], self.weight[:, :, :mx, :my, :mz]\n        )\n        x = torch.fft.irfftn(out_ft, s=(nx, ny, nz))\n        return x\n\n\nclass FNOBlock(nn.Module):\n    def __init__(self, hidden: int, modes: int) -> None:\n        super().__init__()\n        self.spectral = SpectralConv3d(hidden, hidden, modes)\n        self.pointwise = nn.Conv3d(hidden, hidden, kernel_size=1)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.act(self.spectral(x) + self.pointwise(x))\n\n\nclass FNO3D(nn.Module):\n    def __init__(self, in_channels: int, hidden: int, modes: int, layers: int) -> None:\n        super().__init__()\n        self.input_proj = nn.Conv3d(in_channels, hidden, kernel_size=1)\n        self.blocks = nn.ModuleList([FNOBlock(hidden, modes) for _ in range(layers)])\n        self.output_proj = nn.Sequential(\n            nn.Conv3d(hidden, hidden, kernel_size=1),\n            nn.GELU(),\n            nn.Conv3d(hidden, 1, kernel_size=1),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.input_proj(x)\n        for block in self.blocks:\n            x = block(x)\n        return self.output_proj(x)\n\n\ndef surface_area_from_w(w: torch.Tensor, spacing: float, levels: torch.Tensor) -> torch.Tensor:\n    areas = []\n    for level in levels:\n        mask = (w - level).abs() < spacing\n        grad = torch.gradient(w, spacing=(spacing, spacing, spacing))\n        grad_norm = torch.sqrt(sum(g**2 for g in grad) + 1e-8)\n        areas.append((mask * grad_norm).sum() * spacing**2)\n    return torch.stack(areas)\n\n\ndef mdot_choked(cfg: GrainConfig, pc: torch.Tensor) -> torch.Tensor:\n    coeff = math.sqrt(cfg.gamma / (cfg.gas_constant * cfg.chamber_temp))\n    coeff *= (2.0 / (cfg.gamma + 1.0)) ** ((cfg.gamma + 1.0) / (2.0 * (cfg.gamma - 1.0)))\n    return cfg.throat_area * torch.clamp(pc, min=0.0, max=1e8) * coeff\n\n\ndef exhaust_velocity(cfg: GrainConfig, pc: torch.Tensor) -> torch.Tensor:\n    pc_eff = torch.clamp(pc, min=cfg.pa * 1.1, max=1e8)\n    return torch.sqrt(\n        2.0\n        * cfg.gamma\n        * cfg.gas_constant\n        * cfg.chamber_temp\n        / (cfg.gamma - 1.0)\n        * (1.0 - (cfg.pa / pc_eff) ** ((cfg.gamma - 1.0) / cfg.gamma))\n    )\n\n\ndef propellant_volume(phi: torch.Tensor, spacing: float) -> torch.Tensor:\n    return torch.clamp(phi, 0.0, 1.0).sum() * spacing**3\n\n\ndef loading_fraction(phi: torch.Tensor, cfg: GrainConfig, spacing: float) -> torch.Tensor:\n    total = math.pi * cfg.case_radius**2 * cfg.length\n    return propellant_volume(phi, spacing) / total\n\n\ndef sdf_to_occupancy(sdf: torch.Tensor) -> torch.Tensor:\n    return torch.sigmoid(-10.0 * sdf)\n\n\ndef occupancy_smoothness(phi: torch.Tensor) -> torch.Tensor:\n    return (phi * (1.0 - phi)).mean()\n\n\ndef forward_performance(\n    sdf_field: NeuralSDFField,\n    encoder: PositionalEncoding,\n    surrogate: PhysicsSurrogate,\n    coords_flat: torch.Tensor,\n    cfg: GrainConfig,\n    z: torch.Tensor,\n    cond: torch.Tensor,\n) -> Dict[str, torch.Tensor]:\n    enc = encoder(coords_flat)\n    sdf = sdf_field(enc, z, cond).reshape(cfg.grid_size, cfg.grid_size, cfg.grid_size)\n    sdf_grid = sdf.unsqueeze(0).unsqueeze(0)\n    w = surrogate(sdf_grid).squeeze(0).squeeze(0)\n    spacing = max(cfg.length, 2 * cfg.case_radius) / cfg.grid_size\n    levels = torch.linspace(0.0, w.max().detach(), int(cfg.t_end / cfg.dt), device=w.device)\n    areas = surface_area_from_w(w, spacing, levels)\n\n    time_grid = torch.arange(0.0, cfg.t_end + cfg.dt, cfg.dt, device=w.device)\n    pc = torch.tensor(2.0e6, device=w.device)\n    pc_hist = []\n    f_hist = []\n\n    for i in range(len(time_grid)):\n        ab = areas[min(i, len(areas) - 1)]\n        r_dot = cfg.burn_a * torch.clamp(pc, min=1.0, max=1e8) ** cfg.burn_n\n        mdot_gen = cfg.rho_p * ab * r_dot\n        mdot_noz = mdot_choked(cfg, pc)\n        phi = sdf_to_occupancy(sdf)\n        v_g = torch.clamp(\n            (1.0 - phi).sum() * spacing**3,\n            min=1e-6,\n            max=math.pi * cfg.case_radius**2 * cfg.length * 1.1,\n        )\n        dpc_dt = (mdot_gen - mdot_noz) * cfg.gas_constant * cfg.chamber_temp / v_g\n        pc = torch.clamp(pc + dpc_dt * cfg.dt, min=0.0, max=1e8)\n        f_hist.append((mdot_noz * exhaust_velocity(cfg, pc)).item())\n        pc_hist.append(pc.item())\n\n    return {\n        \"time\": time_grid,\n        \"Pc\": torch.tensor(pc_hist, device=w.device),\n        \"F\": torch.tensor(f_hist, device=w.device),\n        \"sdf\": sdf,\n        \"W\": w,\n        \"loading\": loading_fraction(sdf_to_occupancy(sdf), cfg, spacing),\n    }\n\n\ndef reverse_design(\n    target_curve: torch.Tensor,\n    cfg: GrainConfig,\n    diff_cfg: DiffusionConfig,\n    train_cfg: TrainingConfig,\n) -> Dict[str, torch.Tensor]:\n    set_seed(cfg.seed)\n    device = torch.device(cfg.device)\n    _, _, _, coords_flat = make_grid(cfg)\n    coords_flat = coords_flat.to(device)\n\n    encoder = PositionalEncoding().to(device)\n    curve_encoder = CurveEncoder().to(device)\n    cond = curve_encoder(target_curve.unsqueeze(0).unsqueeze(0))\n\n    sdf_field = NeuralSDFField(\n        enc_dim=encoder(coords_flat[:1]).shape[-1],\n        latent_dim=diff_cfg.latent_dim,\n        cond_dim=cond.shape[-1],\n    ).to(device)\n    surrogate = PhysicsSurrogate(cfg.grid_size).to(device)\n    diffusion = LatentDiffusion(diff_cfg, cond_dim=cond.shape[-1]).to(device)\n\n    z = diffusion.sample(cond.squeeze(0))\n    z = nn.Parameter(z)\n\n    optimizer = optim.AdamW(list(sdf_field.parameters()) + [z], lr=train_cfg.lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_cfg.num_iters)\n\n    writer = init_summary_writer(train_cfg, \"reverse_design\")\n    stopper = EarlyStopping(train_cfg.early_stop_patience, train_cfg.early_stop_min_delta)\n    iter_range = range(train_cfg.num_iters)\n    if train_cfg.use_tqdm:\n        iter_range = tqdm(iter_range, desc=\"Reverse optimize\", leave=False)\n\n    for it in iter_range:\n        optimizer.zero_grad()\n        out = forward_performance(sdf_field, encoder, surrogate, coords_flat, cfg, z, cond.squeeze(0))\n        pc = out[\"Pc\"]\n        fit_loss = nnF.mse_loss(pc, target_curve)\n\n        spacing = max(cfg.length, 2 * cfg.case_radius) / cfg.grid_size\n        eikonal_loss = surrogate.eikonal_residual(out[\"W\"], spacing)\n        load_frac = out[\"loading\"]\n        load_loss = (train_cfg.target_loading - load_frac) ** 2\n        load_loss += 10.0 * nnF.relu(train_cfg.min_loading - load_frac) ** 2\n        load_loss += 50.0 * nnF.relu(load_frac - train_cfg.max_loading) ** 2\n\n        loss = 20.0 * fit_loss + train_cfg.pinn_weight * eikonal_loss + train_cfg.loading_weight * load_loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(sdf_field.parameters()) + [z], train_cfg.clip_grad)\n        optimizer.step()\n        scheduler.step()\n\n        rel_err = torch.mean(torch.abs(pc - target_curve) / (target_curve + 1e-6))\n        if writer and it % train_cfg.log_every == 0:\n            writer.add_scalar(\"loss/total\", loss.item(), it)\n            writer.add_scalar(\"loss/fit\", fit_loss.item(), it)\n            writer.add_scalar(\"loss/eikonal\", eikonal_loss.item(), it)\n            writer.add_scalar(\"loss/loading\", load_loss.item(), it)\n            writer.add_scalar(\"metrics/rel_err\", rel_err.item(), it)\n            writer.add_scalar(\"metrics/loading\", load_frac.item(), it)\n        if train_cfg.use_tqdm and hasattr(iter_range, \"set_postfix\"):\n            iter_range.set_postfix(loss=f\"{loss.item():.2e}\", rel_err=f\"{rel_err.item():.3f}\", load=f\"{load_frac.item():.3f}\")\n        if stopper.step(loss.item()):\n            break\n\n    if writer:\n        writer.close()\n\n    final_out = forward_performance(sdf_field, encoder, surrogate, coords_flat, cfg, z, cond.squeeze(0))\n    return {\n        \"Pc\": final_out[\"Pc\"],\n        \"F\": final_out[\"F\"],\n        \"sdf\": final_out[\"sdf\"],\n        \"W\": final_out[\"W\"],\n        \"loading\": final_out[\"loading\"],\n        \"latent\": z.detach(),\n    }\n\n\ndef _build_optimizer_modules(\n    cfg: GrainConfig,\n    diff_cfg: DiffusionConfig,\n    target_curve: torch.Tensor,\n    coords_flat: torch.Tensor,\n) -> Tuple[PositionalEncoding, CurveEncoder, NeuralSDFField, PhysicsSurrogate, LatentDiffusion, torch.Tensor]:\n    encoder = PositionalEncoding().to(cfg.device)\n    curve_encoder = CurveEncoder().to(cfg.device)\n    cond = curve_encoder(target_curve.unsqueeze(0).unsqueeze(0))\n    sdf_field = NeuralSDFField(\n        enc_dim=encoder(coords_flat[:1]).shape[-1],\n        latent_dim=diff_cfg.latent_dim,\n        cond_dim=cond.shape[-1],\n    ).to(cfg.device)\n    surrogate = PhysicsSurrogate(cfg.grid_size).to(cfg.device)\n    diffusion = LatentDiffusion(diff_cfg, cond_dim=cond.shape[-1]).to(cfg.device)\n    return encoder, curve_encoder, sdf_field, surrogate, diffusion, cond.squeeze(0)\n\n\ndef reverse_design_loop(\n    target_curve: torch.Tensor,\n    cfg: GrainConfig,\n    diff_cfg: DiffusionConfig,\n    train_cfg: TrainingConfig,\n) -> Dict[str, torch.Tensor]:\n    \"\"\"STEP 1~3 루프 기반 역설계.\n\n    STEP 1: 성능 요구사항 분석 (조건 임베딩 및 목표 정규화)\n    STEP 2: 최적화 (latent + SDF 필드 + PINN 잔차)\n    STEP 3: SRM 성능 체크 (forward 성능 검증 및 허용오차 평가)\n    \"\"\"\n    set_seed(cfg.seed)\n    device = torch.device(cfg.device)\n    _, _, _, coords_flat = make_grid(cfg)\n    coords_flat = coords_flat.to(device)\n\n    encoder, _, sdf_field, surrogate, diffusion, cond = _build_optimizer_modules(\n        cfg, diff_cfg, target_curve, coords_flat\n    )\n    if train_cfg.cond_dropout > 0:\n        drop_mask = (torch.rand_like(cond) > train_cfg.cond_dropout).float()\n        cond = cond * drop_mask\n\n    z = nn.Parameter(diffusion.sample(cond, guidance_scale=train_cfg.guidance_scale))\n    optimizer = optim.AdamW(list(sdf_field.parameters()) + [z], lr=train_cfg.lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_cfg.inner_iters)\n\n    writer = init_summary_writer(train_cfg, \"reverse_loop\")\n    best = None\n    for step in range(train_cfg.loop_steps):\n        stopper = EarlyStopping(train_cfg.early_stop_patience, train_cfg.early_stop_min_delta)\n        iter_range = range(train_cfg.inner_iters)\n        if train_cfg.use_tqdm:\n            iter_range = tqdm(iter_range, desc=f\"Loop {step + 1}/{train_cfg.loop_steps}\", leave=False)\n        for it in iter_range:\n            optimizer.zero_grad()\n            out = forward_performance(sdf_field, encoder, surrogate, coords_flat, cfg, z, cond)\n            pc = out[\"Pc\"]\n            fit_loss = nnF.mse_loss(pc, target_curve)\n            spacing = max(cfg.length, 2 * cfg.case_radius) / cfg.grid_size\n            eikonal_loss = surrogate.eikonal_residual(out[\"W\"], spacing)\n            load_frac = out[\"loading\"]\n            load_loss = (train_cfg.target_loading - load_frac) ** 2\n            load_loss += 10.0 * nnF.relu(train_cfg.min_loading - load_frac) ** 2\n            load_loss += 50.0 * nnF.relu(load_frac - train_cfg.max_loading) ** 2\n            smooth_loss = occupancy_smoothness(sdf_to_occupancy(out[\"sdf\"]))\n            latent_loss = (z.pow(2).mean())\n\n            loss = (\n                20.0 * fit_loss\n                + train_cfg.pinn_weight * eikonal_loss\n                + train_cfg.loading_weight * load_loss\n                + train_cfg.smooth_weight * smooth_loss\n                + train_cfg.latent_weight * latent_loss\n            )\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(list(sdf_field.parameters()) + [z], train_cfg.clip_grad)\n            optimizer.step()\n            scheduler.step()\n\n            rel_err = torch.mean(torch.abs(pc - target_curve) / (target_curve + 1e-6))\n            global_step = step * train_cfg.inner_iters + it\n            if writer and it % train_cfg.log_every == 0:\n                writer.add_scalar(\"loop/loss_total\", loss.item(), global_step)\n                writer.add_scalar(\"loop/loss_fit\", fit_loss.item(), global_step)\n                writer.add_scalar(\"loop/loss_eikonal\", eikonal_loss.item(), global_step)\n                writer.add_scalar(\"loop/loss_loading\", load_loss.item(), global_step)\n                writer.add_scalar(\"loop/loss_smooth\", smooth_loss.item(), global_step)\n                writer.add_scalar(\"loop/rel_err\", rel_err.item(), global_step)\n                writer.add_scalar(\"loop/loading\", load_frac.item(), global_step)\n            if train_cfg.use_tqdm and hasattr(iter_range, \"set_postfix\"):\n                iter_range.set_postfix(loss=f\"{loss.item():.2e}\", rel_err=f\"{rel_err.item():.3f}\", load=f\"{load_frac.item():.3f}\")\n            if stopper.step(loss.item()):\n                break\n\n        with torch.no_grad():\n            out = forward_performance(sdf_field, encoder, surrogate, coords_flat, cfg, z, cond)\n            rel_err = torch.mean(torch.abs(out[\"Pc\"] - target_curve) / (target_curve + 1e-6))\n            candidate = {\n                \"Pc\": out[\"Pc\"],\n                \"F\": out[\"F\"],\n                \"sdf\": out[\"sdf\"],\n                \"W\": out[\"W\"],\n                \"loading\": out[\"loading\"],\n                \"latent\": z.detach().clone(),\n                \"rel_err\": rel_err,\n                \"loop\": step + 1,\n            }\n            if best is None or rel_err < best[\"rel_err\"]:\n                best = candidate\n            if rel_err <= train_cfg.performance_tol:\n                break\n\n        z = nn.Parameter(diffusion.sample(cond, guidance_scale=train_cfg.guidance_scale))\n        optimizer = optim.AdamW(list(sdf_field.parameters()) + [z], lr=train_cfg.lr)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_cfg.inner_iters)\n\n    if writer:\n        writer.close()\n    return best\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "os.makedirs('out_physgenrd', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: 설정 , 목표 성능 곡선 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GrainConfig()\n",
    "diff_cfg = DiffusionConfig()\n",
    "train_cfg = TrainingConfig()\n",
    "\n",
    "xx, yy, zz, coords_flat = make_grid(cfg)\n",
    "target_curve = build_target_pressure_curve(cfg)\n",
    "\n",
    "encoder = CurveEncoder(target_curve.shape[-1]).to(cfg.device)\n",
    "cond = encoder(target_curve.to(cfg.device))\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(target_curve.cpu().numpy(), label='Target Pc')\n",
    "plt.title('Target Performance Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: 모델 구성 및 기본 진단\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_field = NeuralSDFField(cond.shape[-1]).to(cfg.device)\n",
    "surrogate = PhysicsSurrogate(cfg.grid_size).to(cfg.device)\n",
    "diffusion = LatentDiffusion(diff_cfg, cond_dim=cond.shape[-1]).to(cfg.device)\n",
    "\n",
    "z = diffusion.sample(cond)\n",
    "sdf_values = sdf_field(coords_flat.to(cfg.device), z, cond).reshape(cfg.grid_size, cfg.grid_size, cfg.grid_size)\n",
    "phi = sdf_to_occupancy(sdf_values)\n",
    "spacing = (2 * cfg.case_radius) / (cfg.grid_size - 1)\n",
    "loading = loading_fraction(phi, cfg, spacing)\n",
    "smoothness = occupancy_smoothness(phi)\n",
    "\n",
    "print(f'Loading fraction: {loading.item():.3f}')\n",
    "print(f'Smoothness penalty: {smoothness.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2-1: Forward 성능 예측 및 연소면적 진단\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out = forward_performance(sdf_field, encoder, surrogate, coords_flat, cfg, z, cond)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(forward_out['pressure'].cpu().numpy(), label='Predicted Pc')\n",
    "plt.plot(target_curve.cpu().numpy(), label='Target Pc', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Forward Prediction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out_physgenrd/allinone_forward_pressure.png')\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sdf_grid = sdf_values.unsqueeze(0).unsqueeze(0)\n",
    "    w = surrogate(sdf_grid).squeeze(0).squeeze(0)\n",
    "    levels = torch.linspace(0.0, w.max().item(), steps=50, device=w.device)\n",
    "    area_curve = surface_area_from_w(w, spacing, levels).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(area_curve, label='A(w)')\n",
    "plt.title('Estimated Burning Surface Area')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multi-objective loss (다목적 최적화) ===\n",
    "def web_thickness_stats(w: torch.Tensor):\n",
    "    w_flat = w.flatten()\n",
    "    return w_flat.min(), w_flat.max(), w_flat.mean()\n",
    "\n",
    "def center_stability_penalty(phi: torch.Tensor):\n",
    "    # 중심(0,0,0)에서의 점유율이 낮으면 페널티를 부여하는 간단한 안정성 지표\n",
    "    center_idx = phi.shape[0] // 2\n",
    "    center_val = phi[center_idx, center_idx, center_idx]\n",
    "    return (1.0 - center_val).pow(2)\n",
    "\n",
    "def multi_objective_loss(\n",
    "    pressure_pred: torch.Tensor,\n",
    "    thrust_pred: torch.Tensor,\n",
    "    target_pressure: torch.Tensor,\n",
    "    phi: torch.Tensor,\n",
    "    w: torch.Tensor,\n",
    "    cfg: GrainConfig,\n",
    "    weights: dict,\n",
    "    target_loading: float = 0.6,\n",
    "    min_web: float = 0.01,\n",
    "    max_web: float = 0.2,\n",
    "):\n",
    "    spacing = (2 * cfg.case_radius) / (cfg.grid_size - 1)\n",
    "    loading = loading_fraction(phi, cfg, spacing)\n",
    "    smooth = occupancy_smoothness(phi)\n",
    "    w_min, w_max, _ = web_thickness_stats(w)\n",
    "    web_penalty = torch.relu(min_web - w_min) + torch.relu(w_max - max_web)\n",
    "    stability = center_stability_penalty(phi)\n",
    "    thrust_mse = torch.mean((thrust_pred - thrust_pred.detach()) ** 2)\n",
    "\n",
    "    loss = (\n",
    "        weights['pc'] * torch.mean((pressure_pred - target_pressure) ** 2)\n",
    "        + weights['loading'] * (loading - target_loading).pow(2)\n",
    "        + weights['smooth'] * smooth\n",
    "        + weights['web'] * web_penalty\n",
    "        + weights['stability'] * stability\n",
    "        + weights['thrust'] * thrust_mse\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Constraint enforcement utilities ===\n",
    "def constraint_penalty(\n",
    "    pressure_pred: torch.Tensor,\n",
    "    phi: torch.Tensor,\n",
    "    w: torch.Tensor,\n",
    "    cfg: GrainConfig,\n",
    "    min_load: float,\n",
    "    max_load: float,\n",
    "    max_pressure: float,\n",
    "    min_web: float,\n",
    "    enforce_shell: bool = True,\n",
    "):\n",
    "    spacing = (2 * cfg.case_radius) / (cfg.grid_size - 1)\n",
    "    loading = loading_fraction(phi, cfg, spacing)\n",
    "    load_penalty = torch.relu(min_load - loading) + torch.relu(loading - max_load)\n",
    "    pressure_penalty = torch.relu(pressure_pred.max() - max_pressure)\n",
    "    web_min = w.min()\n",
    "    web_penalty = torch.relu(min_web - web_min)\n",
    "    shell_penalty = torch.tensor(0.0, device=phi.device)\n",
    "    if enforce_shell:\n",
    "        # 케이스 외곽부(최외곽 격자)에는 추진제 점유 금지\n",
    "        shell = torch.cat([\n",
    "            phi[0:1, :, :], phi[-1:, :, :],\n",
    "            phi[:, 0:1, :], phi[:, -1:, :],\n",
    "            phi[:, :, 0:1], phi[:, :, -1:],\n",
    "        ], dim=0)\n",
    "        shell_penalty = shell.mean()\n",
    "    return load_penalty + pressure_penalty + web_penalty + shell_penalty\n",
    "\n",
    "def barrier_loss(violation: torch.Tensor, mu: float = 1e-2):\n",
    "    return -mu * torch.log(torch.clamp(1.0 - violation, min=1e-6))\n",
    "\n",
    "def augmented_lagrangian(\n",
    "    base_loss: torch.Tensor,\n",
    "    constraint_vals: dict,\n",
    "    lambdas: dict,\n",
    "    rho: float = 10.0,\n",
    "):\n",
    "    total = base_loss\n",
    "    for key, g in constraint_vals.items():\n",
    "        lam = lambdas.get(key, torch.tensor(0.0, device=g.device))\n",
    "        total = total + lam * g + 0.5 * rho * g.pow(2)\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Reverse Design (단발 + 루프)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_single = reverse_design(\n",
    "    sdf_field=sdf_field,\n",
    "    encoder=encoder,\n",
    "    surrogate=surrogate,\n",
    "    coords_flat=coords_flat,\n",
    "    cfg=cfg,\n",
    "    diff_cfg=diff_cfg,\n",
    "    train_cfg=train_cfg,\n",
    "    target_curve=target_curve,\n",
    ")\n",
    "\n",
    "reverse_out = reverse_design_loop(\n",
    "    sdf_field=sdf_field,\n",
    "    encoder=encoder,\n",
    "    surrogate=surrogate,\n",
    "    coords_flat=coords_flat,\n",
    "    cfg=cfg,\n",
    "    diff_cfg=diff_cfg,\n",
    "    train_cfg=train_cfg,\n",
    "    target_curve=target_curve,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(reverse_single['pressure'].cpu().numpy(), label='Reverse Pc (single)')\n",
    "plt.plot(reverse_out['pressure'].cpu().numpy(), label='Reverse Pc (loop)')\n",
    "plt.plot(target_curve.cpu().numpy(), label='Target Pc', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Reverse Design Results')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out_physgenrd/allinone_reverse_compare.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FNO 기반 forward/reverse 사용 예시 ===\n",
    "surrogate_core = PhysicsSurrogate(cfg.grid_size).to(cfg.device)\n",
    "z_core = diffusion.sample(cond)\n",
    "core_forward = forward_performance(sdf_field, encoder, surrogate_core, coords_flat, cfg, z_core, cond)\n",
    "core_reverse = reverse_design(\n",
    "    sdf_field=sdf_field,\n",
    "    encoder=encoder,\n",
    "    surrogate=surrogate_core,\n",
    "    coords_flat=coords_flat,\n",
    "    cfg=cfg,\n",
    "    diff_cfg=diff_cfg,\n",
    "    train_cfg=train_cfg,\n",
    "    target_curve=target_curve,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(core_forward['pressure'].detach().cpu().numpy(), label='Core Forward Pc')\n",
    "plt.plot(core_reverse['pressure'].detach().cpu().numpy(), label='Core Reverse Pc')\n",
    "plt.plot(target_curve.cpu().numpy(), label='Target Pc', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('Core FNO Forward/Reverse')\n",
    "plt.tight_layout()\n",
    "plt.savefig('out_physgenrd/allinone_core_forward_reverse.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가/요약 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2) + 1e-8\n",
    "    return 1.0 - ss_res / ss_tot\n",
    "\n",
    "def summarize(tag, out, target):\n",
    "    return {\n",
    "        'tag': tag,\n",
    "        'r2': r2_score(target.to(cfg.device), out['pressure']).item(),\n",
    "        'pressure': out['pressure'].detach().cpu(),\n",
    "        'thrust': out['thrust'].detach().cpu(),\n",
    "        'z': out['z'].detach().cpu(),\n",
    "    }\n",
    "\n",
    "summary = {\n",
    "    'forward': {\n",
    "        'pressure': forward_out['pressure'].detach().cpu(),\n",
    "        'thrust': forward_out['thrust'].detach().cpu(),\n",
    "    },\n",
    "    'reverse_single': summarize('single', reverse_single, target_curve),\n",
    "    'reverse_loop': summarize('loop', reverse_out, target_curve),\n",
    "}\n",
    "\n",
    "torch.save(summary, 'out_physgenrd/allinone_summary.pt')\n",
    "print('Saved summary to out_physgenrd/allinone_summary.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
